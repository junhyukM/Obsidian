## ✅ CLIP 임베딩 방식: "이미지 의미" 중심 + 텍스트와 같은 공간
---
### 🔧 기본 개요
- CLIP은 **이미지-텍스트 쌍**을 기반으로 훈련된 모델
- 목표: 이미지와 문장을 **같은 벡터 공간**에 매핑
    - 예: `"a red apple"` → [0.2, -0.3, …]
    - 이미지(빨간 사과 사진) → 거의 같은 벡터
---
### 📌 CLIP 구성
1. **이미지 인코더**
    - ResNet 계열 또는 ViT (Vision Transformer)
    - 입력 이미지 → `512차원 벡터` 출력
2. **텍스트 인코더**
    - Transformer 계열 (BERT나 GPT 유사)
    - 입력 문장 → `512차원 벡터`
3. **공통 임베딩 공간 학습**
    - 이미지 벡터와 텍스트 벡터가 **같은 의미일수록 가까워지도록** contrastive learning
---
### 🧠 임베딩 추출 과정 (이미지 기준)


```plaintext
이미지 (e.g., 사과 사진)
↓ ViT or ResNet (CLIP 구조 전용)
↓ Projection Layer (512차원 선형변환)
↓ 결과: (1, 512) 임베딩 벡터
```
이 512차원 벡터는 텍스트 벡터와 비교 가능한 **"의미 중심" 벡터**

---
### ✅ 특성
- CLIP은 단순히 시각적 유사성뿐 아니라, **"의미" 중심 유사성**도 파악함
    - 예: 나사와 드라이버는 생긴 건 다르지만, "조립 도구"라는 공통 개념이 있을 경우 더 가까워질 수 있음
- 이미지 임베딩과 텍스트 임베딩이 같은 공간에 존재하기 때문에, 텍스트 검색에도 강력