### 📌 임베딩(Embedding)이란?

- **복잡한 입력 데이터(이미지, 텍스트 등)**를 컴퓨터가 계산하기 좋은 **숫자 벡터**로 변환하는 것
- 예를 들어,
    - 이미지 1장은 RGB 픽셀 수천 개 → **512차원 벡터**
    - 한 문장 → **768차원 벡터**
- 이 벡터는 "그 데이터의 본질적인 특징"을 요약
### 📦 벡터란?

- 그냥 **실수 숫자들의 배열**
- 예: `[0.12, -0.34, 0.98, ..., -0.55]`
- 이 벡터를 공간에 그리면 **하나의 점**으로 표현할 수 있음 (고차원 공간)
### 📍왜 중요한가?

- 벡터끼리는 수학적으로 비교 가능!
    - 거리(distance), 각도(angle), 내적(dot product) 등 계산 가능
- → **비슷한 이미지는 비슷한 벡터가 됨**
    - cosine similarity를 이용해 가까운 벡터를 찾으면, **유사한 이미지**가 됨
- 즉, **벡터 = 이미지의 압축된 의미**

### 🧠 비유: 사람 얼굴 사진

| 사진   | 설명          | 벡터 예시                      |
| ---- | ----------- | -------------------------- |
| 얼굴 A | 남자, 안경, 웃음  | `[0.1, 0.9, -0.2, ...]`    |
| 얼굴 B | 남자, 안경, 무표정 | `[0.12, 0.88, -0.25, ...]` |
| 얼굴 C | 여자, 안경 없음   | `[-0.3, -0.7, 0.4, ...]`   |
|      |             |                            |

→ A와 B는 유사 벡터, A와 C는 거리가 멀다 → 이게 **유사도 검색의 원리**

### 📈 활용
- 유사 이미지 검색
- 이상 탐지 (정상 벡터와 다른 벡터 찾아내기)
- 텍스트-이미지 매칭 (CLIP) 
	- 이건, 텍스트의 임베딩은 다른 모델로 해서 두 임베딩 결과를 매칭하는 방식으로 하는 것으로 보임 
	- 이미지+텍스트 쌍을 기반으로 훈련된 모델
	- 이미지와 문장을 같은 벡터 공간에 매핑
- 군집화, 시각화 (TSNE, PCA 등)

## 언급된 모델

#### [[ResNet50#✅ ResNet50 임베딩 방식 "이미지의 시각적 패턴만" 추출|ResNet50]]
#### [[CLIP#✅ CLIP 임베딩 방식 "이미지 의미" 중심 + 텍스트와 같은 공간|CLIP]]

### 🔍 비교 요약

| 항목            | ResNet50           | CLIP               |
| ------------- | ------------------ | ------------------ |
| **학습 목적**     | 이미지 분류             | 이미지–텍스트 매칭         |
| **입력**        | 이미지                | 이미지                |
| **출력 임베딩 차원** | 2048차원             | 512차원              |
| **임베딩 특징**    | 시각적 패턴 중심          | 의미 중심 (텍스트 호환 가능)  |
| **용도**        | 유사 이미지 검색 (시각적 기준) | 유사 이미지 + 텍스트 기반 검색 |

---
### ✅ 실제로 어떻게 쓰이는가
- 🔧 **ResNet50 임베딩**:
```python
model = torchvision.models.resnet50(pretrained=True) features = model.forward_until_fc_layer(image_tensor)  # 2048차원
```
- 🔗 **CLIP 임베딩**:
```python
model, preprocess = clip.load("ViT-B/32") image_features = model.encode_image(preprocess(image))  # 512차원
```

둘 다 최종 출력은 **numpy 벡터** 
→ cosine similarity 등으로 비교해서 유사 이미지 찾기

### ✋ 정리

| 질문             | 답변                                      |
| -------------- | --------------------------------------- |
| ResNet50 임베딩은? | CNN으로 시각적 특징을 추출해 2048차원 벡터 생성          |
| CLIP 임베딩은?     | 이미지-텍스트 의미 연관성을 학습해 512차원 의미 벡터 생성      |
| 둘의 차이점은?       | ResNet은 "모양 위주", CLIP은 "의미 중심 + 텍스트 연동" |
